# Crawling in Web Development

In programming, crawling (often called web crawling) is the automated process where a program (called a crawler or spider) systematically browses through web pages on the internet.

## How does it work?

- The crawler starts from a list of initial URLs.

- It fetches the HTML content of those pages.

- Then, it scans the page for links and adds those new URLs to its list.

- This process repeats, allowing the crawler to move from one page to another across the web.