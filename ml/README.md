# AI, Machine Learning (ML), and Deep Learning (DL)

- **AI (Artificial Intelligence):** Includes all technologies that mimic human learning, reasoning, and problem-solving.
- **ML (Machine Learning):** A subset of AI that uses data to learn patterns and make predictions.
- **DL (Deep Learning):** A further subset of ML that uses neural networks to learn automatically from large datasets.

## ğŸ·ï¸ Part 1: AI Categories

### âœ… Machine Learning vs Deep Learning

| êµ¬ë¶„          | ë¨¸ì‹ ëŸ¬ë‹ (Machine Learning)                          | ë”¥ëŸ¬ë‹ (Deep Learning)                          |
| ------------- | ---------------------------------------------------- | ----------------------------------------------- |
| ì •ì˜          | ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” AI ê¸°ë²•                   | ì‹ ê²½ë§(Neural Network) ê¸°ë°˜ì˜ í•™ìŠµ              |
| íŠ¹ì§•          | ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ ì°¾ì•„ ì˜ˆì¸¡                          | ë‹¤ì¸µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ       |
| ë°ì´í„° ì˜ì¡´ì„± | â†“ ë‚®ìŒ                                               | â†‘ ë†’ìŒ                                          |
| í•™ìŠµ ì†ë„     | ë¹ ë¦„                                                 | ëŠë¦¼                                            |
| ëª¨ë¸ ì˜ˆì‹œ     | SVM, ëœë¤ í¬ë ˆìŠ¤íŠ¸, KNN, ì„ í˜• íšŒê·€, ë¡œì§€ìŠ¤í‹± íšŒê·€ ë“± | CNN, RNN, LSTM, GAN, Transformer ë“±             |
| ì‘ìš© ë¶„ì•¼     | ì¶”ì²œ ì‹œìŠ¤í…œ, ì§ˆë³‘ ì˜ˆì¸¡, ê¸ˆìœµ ëª¨ë¸ë§                  | ì´ë¯¸ì§€ ì¸ì‹, ìŒì„± ì¸ì‹, ììœ¨ì£¼í–‰, ë²ˆì—­, ìƒì„± AI |

### âœ… Supervised vs Unsupervised Learning

| êµ¬ë¶„        | ì§€ë„í•™ìŠµ (Supervised Learning)                                          | ë¹„ì§€ë„í•™ìŠµ (Unsupervised Learning)                 |
| ----------- | ----------------------------------------------------------------------- | -------------------------------------------------- |
| ì •ì˜        | ì •ë‹µ(ë¼ë²¨)ì´ ìˆëŠ” ë°ì´í„°ë¥¼ í•™ìŠµ                                         | ì •ë‹µ(ë¼ë²¨) ì—†ì´ ë°ì´í„°ë¥¼ í•™ìŠµ                      |
| ëª©ì         | ì…ë ¥ ë°ì´í„°ë¥¼ ë³´ê³  ì •ë‹µì„ ì˜ˆì¸¡                                          | ë°ì´í„°ì˜ ìˆ¨ê²¨ì§„ íŒ¨í„´ì„ ì°¾ìŒ                        |
| ì…ë ¥ ë°ì´í„° | (ì…ë ¥ê°’, ì •ë‹µ) ìŒì´ ì¡´ì¬                                                | ì…ë ¥ê°’ë§Œ ì¡´ì¬ (ì •ë‹µ ì—†ìŒ)                          |
| ì¶œë ¥ ê°’     | íŠ¹ì • ë¼ë²¨(ë¶„ë¥˜) ë˜ëŠ” ìˆ˜ì¹˜ ê°’(íšŒê·€) ì˜ˆì¸¡                                 | ê·¸ë£¹(í´ëŸ¬ìŠ¤í„°) í• ë‹¹ ë˜ëŠ” íŒ¨í„´ ë°œê²¬                 |
| ëŒ€í‘œ ëª¨ë¸   | KNN, SVM, ê²°ì • íŠ¸ë¦¬, ëœë¤ í¬ë ˆìŠ¤íŠ¸, ì„ í˜• íšŒê·€, ë¡œì§€ìŠ¤í‹± íšŒê·€, ì‹ ê²½ë§ ë“± | K-Means, DBSCAN, PCA, êµ°ì§‘ ë¶„ì„, ì—°ê´€ ê·œì¹™ ë¶„ì„ ë“± |
| ì˜ˆì‹œ        | ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜, ì†ê¸€ì”¨ ìˆ«ì ì¸ì‹, ê°€ê²© ì˜ˆì¸¡                             | ê³ ê° ì„¸ë¶„í™”, ì´ìƒ íƒì§€, ì¶”ì²œ ì‹œìŠ¤í…œ                |

## ğŸ·ï¸ Part 2: Libraries

### âœ… Numpy

- ë‹¤ì°¨ì› ë°°ì—´(Matrix)ì˜ ë¹ ë¥¸ ì—°ì‚°

### âœ… Pandas

- ë°ì´í„°ì— ëŒ€í•œ í‘œ í˜•ì‹ì˜ í‘œí˜„

### âœ… Matplotlib

- ë°ì´í„° ê·¸ë˜í”„ ì‹œê°í™” ì²˜ë¦¬

## ğŸ·ï¸ Part 3 : Boxplot

![](img/image.png)

### âœ… Terms of boxplot

| Term                          | Description                                                                                                  |
| :---------------------------- | :----------------------------------------------------------------------------------------------------------- |
| **Minimum**                   | Position 1.5 IQR below the first quartile (Q1)                                                               |
| **First Quartile (Q1)**       | Marks the 25% position at the bottom of the box                                                              |
| **Second Quartile (Q2)**      | Median represented by the line inside the box, indicating 50% position                                       |
| **Third Quartile (Q3)**       | Marks the 75% position at the top of the box                                                                 |
| **Maximum**                   | Position 1.5 IQR above the third quartile (Q3)                                                               |
| **Interquartile Range (IQR)** | The range between Q1 and Q3                                                                                  |
| **Whisker**                   | Extends from the box to indicate the range of the data, up to the smallest and largest values within 1.5 IQR |
| **Outlier**                   | Data points beyond the minimum and maximum; if any exist, they are plotted beyond the whiskers               |

### âœ… boxplot sample code

```py
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(10) # numpy random init
data = np.random.randn(50) * 10
data = np.append(data, [50, -40])

plt.boxplot(data)

plt.title("Box Plot with Outliers")
plt.ylabel("Value")
plt.show()
```

## ğŸ·ï¸ Part 4 : Dataset

| ê°„ë‹¨í•œ ì‹¤ìŠµì„ ìœ„í•´ iris ë°ì´í„°ì…‹ ì‚¬ìš©

### âœ… iris dataset

| Sepal Length (cm) | Sepal Width (cm) | Petal Length (cm) | Petal Width (cm) | Species |
| ----------------- | ---------------- | ----------------- | ---------------- | ------- |
| 5.1               | 3.5              | 1.4               | 0.2              | setosa  |
| 4.9               | 3.0              | 1.4               | 0.2              | setosa  |
| 4.7               | 3.2              | 1.3               | 0.2              | setosa  |
| 4.6               | 3.1              | 1.5               | 0.2              | setosa  |
| 5.0               | 3.6              | 1.4               | 0.2              | set     |

## ğŸ·ï¸ Part 5 : ML

### âœ… Classification : KNN

#### âš™ï¸ sample code

```py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# 1 ì•„ì´ë¦¬ìŠ¤ ë°ì´í„° ë¡œë“œ
iris = load_iris() # 150ê°œ
X = iris.data # íŠ¹ì§• ë°ì´í„° (ê½ƒë°›ì¹¨, ê½ƒìì˜ ê¸¸ì´ì™€ ë„ˆë¹„)
y = iris.target # í’ˆì¢… (0: Setosa, 1: Versicolor, 2: Virginica)

# 2 ë°ì´í„° ë¶„í•  (í›ˆë ¨ ë°ì´í„° 80%, í…ŒìŠ¤íŠ¸ ë°ì´í„° 20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3 ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (KNNì€ ê±°ë¦¬ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì´ë¯€ë¡œ ì •ê·œí™” í•„ìˆ˜)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 4 KNN ëª¨ë¸ í•™ìŠµ (K=5)
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# 5 ì˜ˆì¸¡ ë° í‰ê°€
y_pred = knn.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"KNN ì •í™•ë„: {accuracy:.4f}")

# 6 ìƒ˜í”Œ ë°ì´í„° ì˜ˆì¸¡ (ìƒˆë¡œìš´ ë¶“ê½ƒ ë°ì´í„° ì…ë ¥)
new_sample = np.array([[5.1, 3.5, 1.4, 0.2]]) # Setosaì™€ ìœ ì‚¬í•œ ë°ì´í„°
new_sample_scaled = scaler.transform(new_sample)
predicted_class = knn.predict(new_sample_scaled)
print(f"ì˜ˆì¸¡ëœ í’ˆì¢…: {iris.target_names[predicted_class][0]}")
```

**output**

```
KNN ì •í™•ë„: 1.0000
ì˜ˆì¸¡ëœ í’ˆì¢…: setosa
```

#### âš™ï¸ Summary of KNN

K-Nearest Neighbors (KNN) is a simple and widely used machine learning algorithm. It classifies new data points based on the labels of the K closest data points.

#### âš™ï¸ How KNN Works

1. When a new data point is given, find the K nearest data points in the existing dataset.

2. Determine the most common class (species) among those K points.

3. Assign the new data point to that class.

### âœ… Clustering : K-means

![](img/image-2.png)

#### âš™ï¸ sample code

```py
from sklearn import datasets
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

iris = datasets.load_iris()

X = iris.data[:, 2:]  # Petal length and petal width
y = iris.target  # Actual species labels

kmeans = KMeans(n_clusters=3, random_state=21)  # n_clusters=3 means dividing into 3 clusters
kmeans.fit(X)  # Learn the KMeans model

y_pred = kmeans.labels_  # Cluster labels predicted by KMeans

centers = kmeans.cluster_centers_  # Cluster centroids

fig, axes = plt.subplots(1, 2, figsize=(7, 3))  # Two subplots side by side

axes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='Set1_r', s=10)  # Color by actual labels
axes[0].set_xlabel('Petal length')  # x-axis label
axes[0].set_ylabel('Petal width')  # y-axis label
axes[0].set_title('Actual')  # Title: actual species

axes[1].scatter(X[:, 0], X[:, 1], c=y_pred, cmap='Set1', s=10) # Color by KMeans predictions
axes[1].set_xlabel('Petal length')  # x-axis label
axes[1].set_ylabel('Petal width')  # y-axis label
axes[1].set_title('Predicted')  # Title: predicted clusters for KMeans

axes[1].scatter(centers[:, 0], centers[:, 1], c='blue', marker='x', s=50, label='Centroids')  # Mark centroids
axes[1].legend()  # Show legend

plt.tight_layout()  # Adjust spacing between plots
plt.show() # Display the plots
```

#### âš™ï¸ Summary of K-means

#### âš™ï¸ How K-means Works

1. ì£¼ì–´ì§„ ë°ì´í„°ì…‹ì—ì„œ Kê°œì˜ êµ°ì§‘ì„ ì°¾ëŠ” ì•Œê³ ë¦¬ì¦˜.

2. ì²˜ìŒ Kê°œì˜ ì¤‘ì‹¬ì ì„ ëœë¤ìœ¼ë¡œ ì„ íƒí•˜ê³ , ê° ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ì ì— í• ë‹¹.

3. ê° êµ°ì§‘ì— ì†í•˜ëŠ” ë°ì´í„° í¬ì¸íŠ¸ë“¤ì˜ í‰ê· ì„ ê³„ì‚°í•˜ì—¬ ìƒˆë¡œìš´ ì¤‘ì‹¬ì ì„ ê°±ì‹ .

4. êµ°ì§‘ ì¤‘ì‹¬ì ì´ ë” ì´ìƒ ë³€í•˜ì§€ ì•Šê±°ë‚˜ ì¼ì • ê¸°ì¤€ì„ ë§Œì¡±í•  ë•Œê¹Œì§€ 2ë‹¨ê³„ì™€ 3ë‹¨ê³„ë¥¼ ë°˜ë³µ.

### âœ… Prediction : Linear Regression

![](img/image-1.png)

#### âš™ï¸ sample code

```py
import yfinance as yf
import pandas as pd

stock_data = yf.download('AAPL', start='2020-01-01', end='2025-01-01') #Apple from 2020-1-1 to 2025-1-1
stock_data.head()

import numpy as np
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Convert the date to number (treat dates as ordinal numbers)
stock_data['Date'] = stock_data.index
stock_data['Date'] = stock_data['Date'].map(pd.Timestamp.toordinal)

# Prepare closing prices and date values
X = stock_data['Date'].values.reshape(-1, 1)  # Independent variable: date
y = stock_data['Close'].values  # Dependent variable: closing price

# Train linear regression model
model = LinearRegression()
model.fit(X, y)

# Create future dates to predict (example: up to 7 years from 2020)
future_dates = pd.date_range(start='2020-01-01', periods=365*7, freq='D')
future_dates_ordinal = future_dates.map(pd.Timestamp.toordinal).values.reshape(-1, 1)

# Results from predictiosn
predictions = model.predict(future_dates_ordinal)

# Visualized prices vs predicted prices
plt.figure(figsize=(10, 6))
plt.plot(stock_data.index, stock_data['Close'], label='Actual', color='blue')  # ì‹¤ì œ ì¢…ê°€
plt.plot(future_dates, predictions, label='Predicted', color='red')  # ì˜ˆì¸¡ ì¢…ê°€
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.title('Stock Price Prediction using Linear Regression')
plt.legend()
plt.grid(True)
plt.show()
```

#### âš™ï¸ Summary of Linear Regression

- ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ì„¤ëª…í•˜ê¸° ê°€ì¥ ì í•©í•œ ì§ì„ ì˜ ë°©ì •ì‹ì„ ì°¾ì•„ ë°ì´í„°ë¥¼ ì„¤ëª…í•˜ê±°ë‚˜ ì˜ˆì¸¡
